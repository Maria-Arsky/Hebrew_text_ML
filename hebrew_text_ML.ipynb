{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analyze with ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for reading and writing (input & output) files:\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "import re   # regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show several prints in one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading input files\n",
    "##### The data includes column with short stories in hebrew, and column with the writer's gender (the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "train_filename = '.' + os.sep + 'input' + os.sep + 'annotated_corpus_for_train.xlsx'\n",
    "df_train = pd.read_excel(train_filename, 'corpus', index_col=None, na_values=['NA'])\n",
    "\n",
    "# Untaged test set\n",
    "test_filename  = '.' + os.sep + 'input' + os.sep + 'corpus_for_test.xlsx'\n",
    "df_test  = pd.read_excel(test_filename,  'corpus', index_col=None, na_values=['NA'])\n",
    "\n",
    "# Taged test set\n",
    "my_filename  = '.' + os.sep + 'input' + os.sep + 'taged_test.xlsx'\n",
    "my_test  = pd.read_excel(my_filename,  'corpus', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  בוקר אחד קמתי סהרורי יצאתי מהמיטה קצת מטושטש ,...      m\n",
       "1  לחבר שלי היה יום הולדת וחיפשנו מה אפשר לעשות ל...      m\n",
       "2  השנה האחרונה הייתה שנת קורונה, שנה לא פשוטה בק...      m\n",
       "3  לפני כחצי שנה עברתי לגור בצפון עם בת זוגתי, עב...      m\n",
       "4  יום חמישי רגיל, תמיד מתחיל לעבור טיפה מאוחר יו...      m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_example_id</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>כחלק ממסגרת ההתנדבות שלי במגלה אני הולך לפעמיי...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>לפני שנה החלטתי שאני רוצה להיות טייס, התחלתי ל...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>בתקופת הקורונה של תחילת החיסונים נגד קורונה, א...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>כפי שכולם מכירים או שמעו מחברים עולם הדייטים ה...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>אחת החוויות שהכי זכורות לי, זו החוויה בפרו בטי...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_example_id                                              story\n",
       "0                0  כחלק ממסגרת ההתנדבות שלי במגלה אני הולך לפעמיי...\n",
       "1                1  לפני שנה החלטתי שאני רוצה להיות טייס, התחלתי ל...\n",
       "2                2  בתקופת הקורונה של תחילת החיסונים נגד קורונה, א...\n",
       "3                3  כפי שכולם מכירים או שמעו מחברים עולם הדייטים ה...\n",
       "4                4  אחת החוויות שהכי זכורות לי, זו החוויה בפרו בטי..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>לפני שנים התגוררתי למשך שנה בגרמניה מטרת המעבר...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>ביום הבחירות נסענו לבקר את אימי זל בבית הקברות...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>בשנה אחרונה חוויתי לראשונה את תהליך חיפוש העבו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>אני סטודנט במכללה בסמסטר בשנת תשפא נגשתי למבחן...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>הייתי מדריכה בכפר נוער מתאם הכפר היינו צריכים...</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story gender\n",
       "359  לפני שנים התגוררתי למשך שנה בגרמניה מטרת המעבר...      m\n",
       "360  ביום הבחירות נסענו לבקר את אימי זל בבית הקברות...      m\n",
       "361  בשנה אחרונה חוויתי לראשונה את תהליך חיפוש העבו...      m\n",
       "362  אני סטודנט במכללה בסמסטר בשנת תשפא נגשתי למבחן...      m\n",
       "363   הייתי מדריכה בכפר נוער מתאם הכפר היינו צריכים...      f"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df_train.index:\n",
    "    # replace \" character with non-char (for saving initials)\n",
    "    df_train.iloc[i, 0] = re.sub(r'\"', '', str(df_train.iloc[i, 0]))\n",
    "    # replace special characters with space\n",
    "    df_train.iloc[i, 0] = re.sub(r'\\W', ' ', str(df_train.iloc[i, 0]))\n",
    "    # replace numbers with space\n",
    "    df_train.iloc[i, 0] = re.sub(r'\\d', ' ', str(df_train.iloc[i, 0]))\n",
    "    # replace one-letter word with space\n",
    "    df_train.iloc[i, 0] = re.sub(r'\\s\\w\\s', ' ', df_train.iloc[i, 0])\n",
    "    # remove 'ו' at the beginning of words\n",
    "    df_train.iloc[i, 0] = re.sub(r'\\sו', ' ', df_train.iloc[i, 0])\n",
    "    # replace multiple spaces with single space\n",
    "    df_train.iloc[i, 0] = re.sub(r'\\s+', ' ', df_train.iloc[i, 0], flags=re.I)\n",
    "    \n",
    "df_train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>בוקר אחד סהרורי מהמיטה קצת מטושטש בצעדים כבדים...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>לחבר שלי היה יום הולדת חיפשנו מה אפשר לעשות לו...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>השנה האחרונה הייתה שנת קורונה שנה לא פשוטה בקנ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>לפני כחצי שנה לגור בצפון עם בת בת זוג שלי עברנ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>יום חמישי רגיל תמיד מתחיל לעבור טיפה מאוחר יות...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story gender\n",
       "0  בוקר אחד סהרורי מהמיטה קצת מטושטש בצעדים כבדים...      m\n",
       "1  לחבר שלי היה יום הולדת חיפשנו מה אפשר לעשות לו...      m\n",
       "2  השנה האחרונה הייתה שנת קורונה שנה לא פשוטה בקנ...      m\n",
       "3  לפני כחצי שנה לגור בצפון עם בת בת זוג שלי עברנ...      m\n",
       "4  יום חמישי רגיל תמיד מתחיל לעבור טיפה מאוחר יות...      m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df_train.index:\n",
    "    # replace important words that ends with תי with other words\n",
    "    df_train.iloc[i, 0] = str(df_train.iloc[i, 0]).replace(' אשתי ', ' אישה שלי ')\n",
    "    df_train.iloc[i, 0] = str(df_train.iloc[i, 0]).replace(' אישתי ', ' אישה שלי ')\n",
    "    df_train.iloc[i, 0] = str(df_train.iloc[i, 0]).replace(' זוגתי ', ' בת זוג שלי ')\n",
    "    df_train.iloc[i, 0] = str(df_train.iloc[i, 0]).replace(' ארוסתי ', ' ארוסה שלי ')\n",
    "    df_train.iloc[i, 0] = str(df_train.iloc[i, 0]).replace(' חברתי ', ' חברה שלי ')\n",
    "    df_train.iloc[i, 0] = str(df_train.iloc[i, 0]).replace(' חברותי ', ' חברות שלי ')\n",
    "    df_train.iloc[i, 0] = str(df_train.iloc[i, 0]).replace(' שתי ', ' שתיים ')\n",
    "    # remove all words that ends with תי, for examle: הייתי, אכלתי, הלכתי, ראיתי etc.\n",
    "    df_train.iloc[i, 0] = re.sub(r'[א-ת]+תי\\s', '', df_train.iloc[i, 0])\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the most common words (and add them to the stop words):\n",
    "words = []\n",
    "\n",
    "for i in df_train.index:\n",
    "    split_words = str(df_train.iloc[i, 0]).split()\n",
    "    words = words + split_words\n",
    "\n",
    "counter = Counter(words)\n",
    "common_words = [key for key, _ in counter.most_common(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This two words decreased the accuracy:\n",
    "good_words = ['בת', 'שהיא']\n",
    "\n",
    "for word in good_words:\n",
    "    common_words.remove(word)\n",
    "\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words:\n",
    "\n",
    "stop_words = ['אני','אתה','אנחנו','הם','שלי','שלך','שלנו','שלכם','שלהם','שלהן','לי','לנו','לכם','להם','זה','זאת',\n",
    "'אלה','אלו','תחת','תחחת' 'מתחת','מעל','בין','עם','עד','על','אל','מול','של','אצל','כמו','אחר','אותו','בלי','לפני','אחרי','מאחורי','עלי','עליך',\n",
    "'עלינו','עליכם','עליכן','עליהם','עליהן','כל','כולם','כך','ככה','כזה','אותי','אותם','אותך','אותנו','אתכם','אתכן','איתי','איתו','איתך',\n",
    "'איתם','איתנו','איתכם','איתכן','יהיה','הייתי','היתי','היתה','הייתה','היה','להיות','עצמי','עצמם','עצמן','עצמנו','עצמהם','עצמהן','מי','מה',\n",
    "'איפה','היכן','אם','לאן','איזה','מהיכן','איך','כיצד','מתי','כאשר','למרות','למה','מדוע','כי','יש','אין','אך','מנין','מאין','מאיפה','יכלו',\n",
    "'יוכלו','לא','רק','אולי','לאו','אי','נגד','אף','מצד','בשביל','לבין','באמצע','בתוך','דרך','מבעד','באמצעות','למעלה','למטה','מחוץ',\n",
    "'מן','מין','לעבר','מכאן','כאן','הנה','הרי','פה','שם','שוב','אבל','מבלי','מלבד','בלבד','בגלל','מכיוון','אשר','ואילו','כפי','אז','כן',\n",
    "'לפיכך','מאד','מעט','מעטים','במידה','יותר','מדי','גם','נו','אחרת','אחרים','או','לגמריי','לגמרי','תמיד','היום','מחר','אתמול','שבוע','שנה',\n",
    "'חודש','בנוסף','מעבר','כשהייתי','אילו','לעומת','בעוד','עוד','בניגוד','ככל','מפני','בשל','עקב','בזכות','בכדי','כדי','כדאי','למען',\n",
    "'אכן','אמנם','משמע','כזכור','כבר','בקרוב','לעולם','מעולם','טרם','כמובן','ללא','זמן','טיסה','חול','שום','מכן','קרה','יקרה','קורה','פשוט',\n",
    "'קל','עי','עפ','שעות','שעה','שאר','יום','ימים','חודשים','שנים','קורונה','חולה','חולים','בכל','אפילו','עדיין','למעשה',\n",
    "'בעצם','למחרת','בבוקר','בלילה','בוקר','לילה','בערב','ערב','רואה','רוצה','עושה','הרבה','מצפה','קונה','מצידי','בטח','יתכן','ממש','בזה']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "words = words + common_words + stop_words\n",
    "words = list(set(words))   # without duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that measures the avarage f1 score:\n",
    "similar to f1 macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_avg_calc(tag, predicted):\n",
    "    conf_mat = metrics.confusion_matrix(tag, predicted)\n",
    "\n",
    "    TP = conf_mat[0][0]\n",
    "    FP = conf_mat[0][1]\n",
    "    FN = conf_mat[1][0]\n",
    "    TN = conf_mat[1][1]\n",
    "\n",
    "    precision_f = TP / (TP + FP)\n",
    "    recall_f = TP / (TP + FN)\n",
    "\n",
    "    precision_m = TN / (TN + FN)\n",
    "    recall_m = TN / (TN + FP)\n",
    "\n",
    "    F1_f = 2 * (precision_f * recall_f) / (precision_f + recall_f)\n",
    "    F1_m = 2 * (precision_m * recall_m) / (precision_m + recall_m)\n",
    "\n",
    "    F1_avg = (F1_f + F1_m) / 2\n",
    "    return F1_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the train into train and test (validation set) for 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v, X_test_v, y_train_v, y_test_v = train_test_split(df_train.story, df_train.gender, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best algorithm:\n",
    "We want to find an algorithm which returns a close f1 score for both the valid set and the test set.\n",
    "#### NB gives the best results.\n",
    "I tried different hyperparameters for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nearest Neighbors'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=7, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=7, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45811240721102864"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4885991112406207"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear SVM'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='linear', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='linear', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6023002421307506"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6581196581196581"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Decision Tree'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                        criterion='gini', max_depth=15,\n",
       "                                        max_features=None, max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort='deprecated', random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                        criterion='gini', max_depth=15,\n",
       "                                        max_features=None, max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort='deprecated', random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6256410256410255"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5772903225806452"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Neural Net'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                               batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
       "                               early_stopping=False, epsilon=1e-08,\n",
       "                               hidden_layer_sizes=(100,),\n",
       "                               learning_rate='constant',\n",
       "                               learning_rate_init=0.001, max_fun=15000,\n",
       "                               max_iter=1000, momentum=0.9, n_iter_no_change=10,\n",
       "                               nesterovs_momentum=True, power_t=0.5,\n",
       "                               random_state=None, shuffle=True, solver='adam',\n",
       "                               tol=0.0001, validation_fraction=0.1,\n",
       "                               verbose=False, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                               batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
       "                               early_stopping=False, epsilon=1e-08,\n",
       "                               hidden_layer_sizes=(100,),\n",
       "                               learning_rate='constant',\n",
       "                               learning_rate_init=0.001, max_fun=15000,\n",
       "                               max_iter=1000, momentum=0.9, n_iter_no_change=10,\n",
       "                               nesterovs_momentum=True, power_t=0.5,\n",
       "                               random_state=None, shuffle=True, solver='adam',\n",
       "                               tol=0.0001, validation_fraction=0.1,\n",
       "                               verbose=False, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6862809917355371"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6637711449284829"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AdaBoost'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "                                    learning_rate=1.0, n_estimators=50,\n",
       "                                    random_state=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "                                    learning_rate=1.0, n_estimators=50,\n",
       "                                    random_state=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5207070707070708"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6469827586206897"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Naive Bayes'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=2000, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7482758620689656"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for test:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.724398416082851"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Decision Tree\",\n",
    "         \"Neural Net\", \"AdaBoost\", \"Naive Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(7),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    DecisionTreeClassifier(max_depth=15),\n",
    "    MLPClassifier(alpha=0.0001, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    MultinomialNB()]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    name\n",
    "    \n",
    "    text_clf_train = Pipeline([('vect', CountVectorizer(max_features=2000)), ('clf', clf)])\n",
    "    text_clf_test = Pipeline([('vect', CountVectorizer(max_features=2000)), ('clf', clf)])\n",
    "\n",
    "    text_clf_train.fit(X_train_v, y_train_v)\n",
    "    predicted_train_v = text_clf_train.predict(X_test_v)\n",
    "    \n",
    "    text_clf_test.fit(df_train.story, df_train.gender)\n",
    "    predicted_test = text_clf_test.predict(df_test.story)\n",
    "    \n",
    "    print(\"for train:\")\n",
    "    f1_avg_calc(y_test_v, predicted_train_v)\n",
    "    \n",
    "    print(\"for test:\")\n",
    "    f1_avg_calc(my_test.tag, predicted_test)\n",
    "    \n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the best parameters I found gave me avg_f1 = 0.7674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=4500, min_df=1,\n",
       "                                 ngram_range=(1, 6), preprocessor=None,\n",
       "                                 stop_words=['כך', 'שלנו', 'למעלה', 'קרה',\n",
       "                                             'עליהן', 'להם', 'אחת', 'אין',\n",
       "                                             'אבל', 'לאחר', 'טרם', 'איתך',\n",
       "                                             'מבלי', 'אתכם', 'אוכל', 'פתאום',\n",
       "                                             'אנשים', 'השנה', 'מפני', 'שעה',\n",
       "                                             'אכן', 'שבוע', 'אחרת', 'מחוץ',\n",
       "                                             'דבר', 'שלו', 'אמר', 'כשהייתי',\n",
       "                                             'לחזור', 'הקורונה', ...],\n",
       "                                 strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.767400250492038"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_nb = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=words, ngram_range=(1,6), max_features=4500)),\n",
    "    ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf_nb.fit(df_train.story, df_train.gender)\n",
    "predicted_test = text_clf_nb.predict(df_test.story)\n",
    "\n",
    "print(\"f1-score:\")\n",
    "f1_avg_calc(my_test.tag, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_example_id</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_example_id predicted_category\n",
       "0                0                  m\n",
       "1                1                  f\n",
       "2                2                  m\n",
       "3                3                  f\n",
       "4                4                  m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted = df_test.copy()\n",
    "\n",
    "df_predicted.drop(labels='story', axis=1, inplace=True)\n",
    "df_predicted['predicted_category'] = predicted_test\n",
    "\n",
    "df_predicted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted.to_csv('classification_results.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
